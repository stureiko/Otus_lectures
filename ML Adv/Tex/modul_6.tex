\chapter{Модуль 6. Bayesian Learning}
\section{Введение в вероятностное моделирование}
\section{Markov Chain Monte-Carlo (MCMC), Metropolis–Hastings}
\subsection{Постановка задачи обучения модели в Байесовском подходе}
У нас есть модель с параметрами $\theta$ и переменными $x, y$. 

Что такое обучить Байесовскую модель - найти постериор, т.е распределение параметров $\theta$, при условии что мы пронаблюдали наши данные.
$$p(\theta|x, y) = \frac{p(y|x, \theta)p(\theta)}{p(x, y)} = \frac{p(y|x, \theta)p(\theta)}{\int\limits_{\theta}p(y|x, \theta)p(\theta)\mathrm d \theta} \to Posterior = \frac{Likelihood \to Prior}{Evidence}$$ 
где: $x, y$ - входы и выходы,

$\theta$ - параметры модели,

$p(y|x, \theta)$ - вероятность пронаблюдать такие выходы при заданных параметрах и входах (Likelihood), 

$p(\theta)$ - априорное распределение на параметры модели, 

$p(x, y)$ - вероятность пронаблюдать такие данные при любых параметрах (Evidence), 

$p(\theta|x, y)$ - вероятность распределения на параметры при наблюдаемых данных (Posterior)

$\displaystyle \int\limits_{\theta}p(y|x, \theta)p(\theta)\mathrm d \theta$ - называется нормализующая константа.

\textbf{Что значит обучить модель}

перепишем в терминах ML:
$$p(\theta|X_{tr}, Y_{tr}) = \frac{p(Y_{tr}|X_{tr}, \theta)p(\theta)}{\int p(Y_{tr}|X_{tr}, \theta)p(\theta) \mathrm d \theta}$$

Т.е мы ищем множество гипотез с различными параметрами $\theta$ которые потом агрегируются с различными весами в единую модель. Фактически мы обучаем не одну модель, а ансамбль моделей.

\textbf{Получить предсказание}

У нас на входе есть новые данные $(x)$, и апостериорное распределение $p(\theta|X_{tr}, Y_{tr})$, которое мы получили при обучении модели. И мы ищем распределение на $y$ при заданных данных, фактически мы ищем мат ожидание $y$ по параметрам.
$$p(y|x, X_{tr}, Y_{tr}) = \int \limits_\theta p(y|x, \theta)p(\theta|X_{tr}, Y_{tr}) \mathrm d \theta$$

\subsection{Сэмплирование}

На практике взять интеграл $\int \limits_\theta p(y|x, \theta)p(\theta|X_{tr}, Y_{tr}) \mathrm d \theta$ практически не возможно. 

В первую очередь потому что размерность пространства $\Theta$ как правило велика. Велика настолько, что численные методы интегрирования не эффективны.

Если взять выборку по параметрам, взять частичную сумму и усреднить по ней - то мы получим оценку близкую к результату.

$$p(y|x, X_{tr}, Y_{tr}) \simeq \sum_{\theta_{sample}}p(y, x, \theta_{sample})p(\theta_{sample}| X_{tr}, Y_{tr})$$

Возникает вопрос - как взять выборку из распределения?

В общем виде нам необходимо оценить наш стохастический интеграл, т.е получить мат ожидание следующего вида:

$$\mathbb{E}_x$$