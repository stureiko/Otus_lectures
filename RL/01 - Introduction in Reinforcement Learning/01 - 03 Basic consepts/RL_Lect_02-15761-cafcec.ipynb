{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNabRJfMp0UT"
      },
      "source": [
        "### Обучение с подкреплением\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cPSeYcCp0UT"
      },
      "source": [
        "В некоторых случаях существующие методы обучения без учителя нам не подходят. В то же время у нас нет возможности создать качественную обучающую выборку. При этом мы можем постфактум оценить действия нашей модели и, используя эту оценку, подстроить модель так, чтобы она чаще совершала желательные действия и реже &mdash; нежелательные. В литературе такую оценку называют вознаграждением (reward). В этом случае обучение строится таким образом, чтобы модель стремилась максимизировать получаемое вознаграждение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ntInTsNp0UU"
      },
      "source": [
        "<center><img src=\"https://edunet.kea.su/repo/EduNet-content/L15/out/branches_of_machine_learning.png\" alt=\"Drawing\" width=\"350\"></center>\n",
        "\n",
        "\n",
        "<center><em> Branches of Machine Learning </em></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6uxKZkWp0UV"
      },
      "source": [
        "## Терминология: агент, функция награды, состояние среды\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIIhtiBop0UV"
      },
      "source": [
        "Агент и среда — ключевые понятия в обучении с подкреплением.\n",
        "\n",
        "**Агент** — программа, принимающая решение о дальнейших действиях на основе информации о состоянии среды."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pM-pVutp0UW"
      },
      "source": [
        "**Среда** — это мир, в котором агент должен \"выживать\", т.е. всё, с чем агент может прямо или косвенно взаимодействовать. Среда обладает состоянием (State), агент может влиять на среду, совершая какие-то действия (Actions), переводя среду при этом из одного состояния в другое и получая какое-то вознаграждение. Среда описывается пространством возможных состояний. Конкретное состояние — вектор в этом пространстве.\n",
        "\n",
        "В зависимости от конкретной задачи, агент может наблюдать либо полное состояние среды, либо только некоторую его часть. Во втором случае агенту может потребоваться какое-то внутреннее представление полного состояния, которое будет обновляться по мере получения новых данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMb5C5cpp0UW"
      },
      "source": [
        "**Функция награды** — вводимая программистом формула вычисления ценности действия на основе финального результата, ожидания этого результата, промежуточных результатов и любых других параметров, которые будут подсказывать путь к наилучшей последовательности действий агента. Это некоторый аналог функции потерь, без которой непонятно, чему учиться. Например, в шахматах истинная награда — это победа, но взятая фигура соперника тоже ценна и должна увеличивать награду, если мы хотим подсказать агенту, что брать чужие фигуры полезно. Может ли после этого агент получать мат, позарившись на незащищенную фигуру? Да, ровно как и неопытный шахматный игрок.\n",
        "\n",
        "Попытка передать через дополнительные неосновные награды подсказки к получению основной награды называется **reward shaping**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isyfrJMOp0UW"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L15/rl_scheme.png\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0q14sPJp0UX"
      },
      "source": [
        "- Неопределенность: переходы и награды случайны\n",
        "- Отложенность награды может не зависеть от действия"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDGx5ERTp0UX"
      },
      "source": [
        "## Классические примеры задач RL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NSvu97Sp0UX"
      },
      "source": [
        "* Обучение автопилота вождению машины\n",
        "* Управление роботами, БПЛ\n",
        "* Обучение нейронной сети игре в игры/видеоигры\n",
        "* Составление расписаний с помощью нейросети\n",
        "* Рекомендательные системы\n",
        "* Трейдинг, принятие решений в условиях неопределенности, управление инвестициями\n",
        "* Drug discovery: есть много свойств химических веществ, которые подсчитываются специальными программами. Выходы, которые, очевидно, недиффиренцируемы. Если мы хотим генерировать вещества с заданными свойствами, то один из подходов — RL\n",
        "* Не так давно одна российская металлургическая компания использовала RL для оптимизации работы прокатного станка\n",
        "* NAS (Neural Architecture Search): при поиске оптимальных топологий сети можно решать задачу с помощью RL, награждая нашего агента за нахождение хороших топологий и наказывая за плохие\n",
        "\n",
        "\n",
        "Во всех этих задачах невозможно или непрактично собирать и размечать датасет, поэтому используется обучение с подкреплением.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wukf0yaQp0U0"
      },
      "source": [
        "Сделаем также важное предположение о природе нашего процесса: **он полностью описывается своим текущим состоянием**. Все, что произойдет в будущем, не зависит от информации из прошлого, кроме той, что мы уже наблюдаем в настоящем. Такой процесс называется __марковским__.\n",
        "\n",
        "\n",
        "Приведем классические примеры марковского процесса:\n",
        "\n",
        "1. Игральный кубик. Мы знаем, что на нем выпадет любая из граней с некоторой фиксированной вероятностью. На это никак не влияет то, что до этого на кубике выпало 6 шестерок подряд. Это может повлиять на нашу оценку вероятностей выпадения той или иной грани, но не на реальную вероятность.\n",
        "\n",
        "2. Шахматы. \"Текущая позиция на доске + чей ход\" однозначно описывает игру.\n",
        "\n",
        "3. А подходит ли покер?\n",
        "\n",
        "С одной стороны, да. Текущее количество денег у каждого из игроков однозначно описывает игру. Но это если мы не учитываем блеф и другие факторы, которые могут сработать / не сработать в зависимости от предыдущих ситуаций в игре. То есть, все зависит от того, как именно мы будем описывать покер и какие допущения накладываем.\n",
        "\n",
        "И получение хорошего представления S — тоже важная задача."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXBue9fQp0U1"
      },
      "source": [
        "### Определение Markov Property"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5VCHGncp0U1"
      },
      "source": [
        "\"*The future is independent of the past given the present*\"\n",
        "\n",
        "Состояние $S_{t}$ является Марковым тогда и только тогда, когда:\n",
        "$$ \\large\n",
        "p\\left(r_{t}, s_{t+1} \\mid s_{0}, a_{0}, r_{0}, \\ldots, s_{t}, a_{t}\\right)=p\\left(r_{t}, s_{t+1} \\mid s_{t}, a_{t}\\right)\n",
        "$$\n",
        "\n",
        "**Что происходит потом, зависит только от предыдущего состояния**\n",
        "- Вся актуальная информация берется из истории.\n",
        "- Как только состояние среды станет известно, историю можно выбросить.\n",
        "- То есть, состояние среды — достаточная статистика для будущего обучения.\n",
        "\n",
        "Вероятность перехода в новое состояние зависит только от текущего состояния и текущего действия."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-41yrG75p0U1"
      },
      "source": [
        "Марковский процесс описывает **полностью наблюдаемые (fully observable)** среды. Можно описать их так:\n",
        "\n",
        "* Информации, которую получает агент в момент времени t, достаточно, чтобы принять оптимальное решение для момента времени t.\n",
        "* Текущее состояние среды содержит всю релевантную информацию из прошлого.\n",
        "* Текущее состояние — это достаточная статистика будущего $\\rightarrow$ не существует никакой дополнительной информации, которая могла бы улучшить наше описание будущего."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQUgYMmjp0U1"
      },
      "source": [
        "## Markov process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEes91pdp0U1"
      },
      "source": [
        "Представим себе, что студент живет вот по такой схеме. Заметим, что влиять в такой схеме на свои решения он не может: все решается подкидыванием кубика."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1R9EPoMp0U1"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/EduNet-content/L15/out/markov_process.png\" width=\"900\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6S5bAVjp0U1"
      },
      "source": [
        "### Определение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhQxe0-ep0U1"
      },
      "source": [
        "Марковский процесс (цепь) — это кортеж $(S, P)$, где\n",
        "- $S$ — принимает дискретные (конечные) значения,\n",
        "- $P$ — матрица переходов (transition matrix):\n",
        "$$\\large\n",
        "P_{s s^{\\prime}}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s\\right)\n",
        "$$\n",
        "\n",
        "Строго говоря, необходимо еще распределение начальных состояний (но мы предполагаем, что оно вырождено, т.е. мы знаем, где начинаем, с вероятностью 1).\n",
        "\n",
        "Марковский процесс — основа для RL. Мы будем постепенно усложнять эту модель, добавляя rewards и actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBVobLmJp0U2"
      },
      "source": [
        "В определении нам встретилась матрица переходов, зададим ее тоже формально."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WaUxVhrp0U2"
      },
      "source": [
        "## Матрица состояний\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfpwyKl8p0U2"
      },
      "source": [
        "### Transition matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t8Xp3BNp0U2"
      },
      "source": [
        "Пусть $S_{t}$ — последовательность дискретных состояний.\n",
        "\n",
        "Поскольку последовательность задается распределением $\\operatorname{Pr}\\left(S_{t+1} \\mid S_{t}\\right)$, естественно упорядочить его в матрицу $ P_{s s^{\\prime}}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s\\right)$\n",
        "\n",
        "$$\n",
        "\\mathcal{P}=\\text { from }\\left[\\begin{array}{ccc}\n",
        "\\mathcal{P}_{11} & \\ldots & \\mathcal{P}_{1 n} \\\\\n",
        "\\vdots & & \\\\\n",
        "\\mathcal{P}_{n 1} & \\ldots & \\mathcal{P}_{n n}\n",
        "\\end{array}\\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu6D4F5_p0U2"
      },
      "source": [
        "## Награда (Reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtmTHTz9p0U2"
      },
      "source": [
        "Наши состояния, очевидно, неравноценны. Давайте к каждому состоянию привяжем награду $R_s$.\n",
        "Она может быть как нулевой, так положительной либо отрицательной.\n",
        "\n",
        "Например, в марковском процессе принятия решений (MDP), описывающем игру в шахматы или GO, награда агента может быть положительной только при переходе в состояние \"партия выиграна\"\n",
        "\n",
        "На всех остальных ходах награда будет нулевой.\n",
        "\n",
        "Для перемещающегося робота награда на каждом шаге может быть отрицательной, так как на перемещение требуется энергия и т.п.\n",
        "\n",
        "Для покера наградой можно назначить изменение текущей суммы игрока после сыгранной раздачи.\n",
        "\n",
        "Схема для студента в этом случае модифицируется следующим образом. Заметьте, награды тут расставляются с точки зрения актора — студента:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKOfQHxdp0U3"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/EduNet-content/L15/out/markov_reward.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXkBCjoYp0U3"
      },
      "source": [
        "## Суммарная награда (Return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvybqyFlp0U3"
      },
      "source": [
        "Для турнира по шахматам, где играется 10 партий, финальный reward будет складываться из reward за каждую партию в отдельности.\n",
        "\n",
        "Если у нас есть тест из 40 вопросов, то опять же, нам не важно, как именно мы получили 30 баллов, главное — мы их получили.\n",
        "\n",
        "То есть, в таких случаях мы можем считать Return следующим образом:\n",
        "\n",
        "$$Return = \\sum_i {R_i}$$\n",
        "\n",
        "\n",
        "А теперь представим себе, что мы можем работать 2 года бесплатно и получить разом 5 миллионов рублей. А можем работать 2 года и каждый год получать 70 тысяч.\n",
        "\n",
        "\n",
        "Или мы можем 6 лет учиться, получая сторонними активностями 20 тысяч в месяц, а затем сразу начать получать 150 тысяч в месяц, а затем и больше. А можем сразу пойти на работу и начать зарабатывать 50 тысяч в месяц, постепенно дойдя до 100.\n",
        "\n",
        "Что лучше: опубликовать две статьи в журнале с импакт фактором 5 в этом году или 1, но через 2 года и в журнале с импакт-фактором 20?\n",
        "\n",
        "Что выгоднее?\n",
        "\n",
        "Очевидно, в таких ситуациях лучше учитывать не только Reward, но и то, когда он получен."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfyHsbXxp0U3"
      },
      "source": [
        "### Дисконтирование (discounting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVlQ3cs-p0U3"
      },
      "source": [
        "Поэтому при оценке кумулятивной награды на шаге $t$ $(G_t)$ используется коэффициент дисконтирования $\\gamma$, который показывает, насколько ценными являются будущие награды на текущий момент (см. [Markov Decision Processes (David Silver Lectures)](https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf)):\n",
        "\n",
        "$$\\large G_t = R_{t+1} + \\gamma R_{t+2} + ... =\\sum^{\\infty}_{k=0} \\gamma ^ kR_{t+k+1},\n",
        "$$\n",
        "\n",
        "где:\n",
        "- $R_{t+1}, R_{t+1}, \\dots$ — вознаграждения по рёбрам переходов,\n",
        "- дисконтирование $\\gamma \\in[0,1]$ — это текущая стоимость будущих вознаграждений,\n",
        "- $\\gamma^{k} R$ — ценность получения награды $R$ после $k+1$ шагов.\n",
        "\n",
        "При этом:\n",
        "- немедленное вознаграждение ценится выше отложенного вознаграждения,\n",
        "- $\\gamma$ близко к 0 приводит к «близорукости»,\n",
        "- $\\gamma$ близко к 1 приводит к «дальновидной» оценке."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJXTmCbwp0U3"
      },
      "source": [
        "\n",
        "Какое значение $\\gamma$ выбрать?\n",
        "\n",
        "\n",
        "**Можно ли его выбрать равным 1?**\n",
        "\n",
        "Да, мы уже это делали ранее.\n",
        "\n",
        "**Можно ли его выбрать равным 0?**\n",
        "\n",
        " Тоже да. В этом случае у нас получится \"жадный\" алгоритм: мы всегда выбираем решение, которое дает максимальную награду сейчас, нас не волнуют будущие награды.\n",
        "\n",
        "**Может $\\gamma = 0$ привести к проблемам?**\n",
        "\n",
        "Да, например:\n",
        "\n",
        "* Вариант 1: Получить сегодня 1000 рублей.\n",
        "\n",
        "* Вариант 2: Получить завтра 100000 рублей\n",
        "\n",
        "Обычно, второй вариант более предпочтителен. Но жадный алгоритм его не увидит.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_OP8X8Np0U4"
      },
      "source": [
        "**Может $\\gamma = 1$ привести к проблемам?**\n",
        "\n",
        "Представим себе генератор шуток про медведя. Большая часть людей знает эту шутку, поэтому было бы хорошо посередине вставлять дополнительные детали &mdash; \"для ценителей\". Но заканчиваться шутка должна одинаково.\n",
        "\n",
        "Мы решили написать программу, которая генерирует шутки про медведя. Шутка должна начинаться с \"Шел медведь по лесу\" и заканчиваться \"Сел в машину и сгорел\". Как бы это описать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "095Q0sGMp0U4"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L15/cat_comedian.png\" alt=\"Drawing\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4orqO6Ocp0U4"
      },
      "source": [
        "Есть состояния, а модель сама решает, куда ей переходить. Вероятностей мы тут не добавили. Что модель решила, то и будет.\n",
        "\n",
        "Сразу видим и проблему, которую мы получаем при $\\gamma = 1$, пусть и в утрированном виде. Ничто не запрещает нашей модели бесконечно добавлять детали к анекдоту, тем самым увеличивая $G_t$.\n",
        "Работать с такой моделью невозможно.\n",
        "\n",
        "Если в какой-то момент обучения наша модель стала походить на эту, то ничего осмысленного мы и далее не получим. А вот если введением дискаунта дать модели понять, что слушатели анекдота все же хотят вскоре услышать его конец, то проблема уйдет."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLoAEBKAp0U4"
      },
      "source": [
        "**Обычно $0 \\le \\gamma \\le 1$**\n",
        "\n",
        "Обычно $\\gamma$ ставят равной чему-то между двумя этими крайностями.\n",
        "\n",
        "Близость $\\gamma$ к 0 отражает нашу \"нетерпеливость\" — насколько важно получить награду именно сейчас.\n",
        "\n",
        "Наличие такого $\\gamma$ позволяет нам не делать различия между моделями с ограниченным числом шагов и неограниченным: теперь в любом случае return будет конечным числом, т.к. выражение для return ограничено сверху суммой бесконечно убывающей геометрической прогрессии.\n",
        "\n",
        "Пусть $R_{max} = max R_i$ — максимальная награда, которую мы в принципе можем получить в каком-то состоянии.\n",
        "\n",
        "$\\displaystyle G_t = R_{t+1} + \\gamma \\cdot R_{t+2} + \\gamma^2 \\cdot R_{t+3} + ... \\le R_{max} + \\gamma \\cdot R_{max} + \\gamma^2 \\cdot R_{max} + ... = R_{max} (1 + \\gamma + \\gamma^2 + ... ) \\le R_{max} \\cdot \\frac{1}{1-\\gamma} = const $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIf4_8_3p0U4"
      },
      "source": [
        "**Discounting makes sums finite**\n",
        "\n",
        "<img src=\"https://edunet.kea.su/repo/EduNet-content/L15/out/discounting_makes_sums_finite.png\" alt=\"Drawing\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytV4_o7-p0U4"
      },
      "source": [
        "\\begin{equation}\n",
        "G_{0}=\\sum_{k=0}^{\\infty} \\gamma^{k}=\\frac{1}{1-\\gamma}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz7U5cctp0U5"
      },
      "source": [
        "Важно понимать, что выбор $\\gamma$ меняет задачу, которую мы решаем, и, соответственно, меняет решение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDzHoJPp0U5"
      },
      "source": [
        "### Markov reward process (MRP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZHU_uHOp0U5"
      },
      "source": [
        "MRP — это кортеж $(S, R, P, \\gamma)$, где:\n",
        "\n",
        "$S$ — дискретное состояние, принимает дискретные (конечные значения),\n",
        "\n",
        "$R$ — функция rewards, $R_s = \\mathbb{E}[R_{t+1}|S_t=s]$,\n",
        "\n",
        "$P$ — матрица переходов (transition matrix):\n",
        "\n",
        "$P_{ss^`}=Pr(S_{t+1}=s^`|S_t=s)$,\n",
        "\n",
        "$\\gamma$ — коэффициент дисконтирования (discount factor).\n",
        "\n",
        "Осталось добавить actions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ges8wk1p0U5"
      },
      "source": [
        "# Марковский процесс принятия решений\n",
        "Markov decision process (MDP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZNnpj_Tp0U5"
      },
      "source": [
        "До этого у нас получалось малость неадекватное представление процесса &mdash; студент не мог ни на что повлиять. На самом же деле студент может решать, куда ему надо: в аудиторию, домой спать или в бар. Однако наше действие не всегда определяет состояние, в которое мы перейдем."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GFbs4slp0U5"
      },
      "source": [
        "Хочет наш студент пойти на лекцию, но по пути встречает товарища, и идут они в бар. Хочет он пойти спать, а по пути к выходу из университета встречает лектора с хорошей памятью и идет на лекцию.\n",
        "И т.д.\n",
        "\n",
        "Как это отразить на схеме так, чтобы это можно было прочитать? Введем промежуточные состояния, куда нас переводят действия студента. А уже из этих промежуточных состояний случайно будем переходить в состояния среды.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev1mi6KIp0U5"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/EduNet-content/L15/out/markov_decision_process_return_random.png\" alt=\"Drawing\" width=\"650\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmevnMR9p0U6"
      },
      "source": [
        "## Формальное описание MDP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obIm5qH_p0U6"
      },
      "source": [
        "MDP — это кортеж $(S, A, R, P, \\gamma)$, где:\n",
        "- $S$ — состояния (дискретное пространство),\n",
        "- $A$ — действия (дискретное пространство),\n",
        "- $R$ — функция rewards: $R_{s}^{a}=\\mathbb{E}\\left[R_{t+1} \\mid S_{t}=s, A_{t}=a\\right]$,\n",
        "- $P$ — матрица переходов (transition matrix): $P_{s s^{\\prime}}^{a}=\\operatorname{Pr}\\left(S_{t+1}=s^{\\prime} \\mid S_{t}=s, A_{t}=a\\right)$,\n",
        "- $\\gamma$ — discount factor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa160KTep0U6"
      },
      "source": [
        "### Пример"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1L3IJNkp0U6"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/EduNet-content/L15/out/markov_decision_process_example.png\" alt=\"Drawing\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IITwOFDCp0U6"
      },
      "source": [
        "- черные беззнаковые числа — вероятности переходов;\n",
        "- знаковые — это награды."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13RzSOEUrBDU"
      },
      "source": [
        "\n",
        "Приведем другие классические примеры марковского процесса:\n",
        "\n",
        "1. Игральный кубик. Мы знаем, что на нем выпадет любая из граней с некоторой фиксированной вероятностью. На это никак не влияет то, что до этого на кубике выпало 6 шестерок подряд. Это может повлиять на нашу оценку вероятностей выпадения той или иной грани, но не на реальную вероятность.\n",
        "\n",
        "2. Шахматы. \"Текущая позиция на доске + чей ход\" однозначно описывает игру.\n",
        "\n",
        "3. А подходит ли покер?\n",
        "\n",
        "С одной стороны, да. Текущее количество денег у каждого из игроков однозначно описывает игру. Но это если мы не учитываем блеф и другие факторы, которые могут сработать / не сработать в зависимости от предыдущих ситуаций в игре. То есть, все зависит от того, как именно мы будем описывать покер и какие допущения накладываем.\n",
        "\n",
        "И получение хорошего представления S — тоже важная задача."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEoBnAJup0U7"
      },
      "source": [
        "## Нахождение лучшей последовательности переходов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1PWQksIp0U7"
      },
      "source": [
        "Нужно найти последовательность переходов, которой будет соответствовать максимальная награда ($G_t$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3a-SM7-p0U7"
      },
      "source": [
        "### Определение Политики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEvCjfoAp0U8"
      },
      "source": [
        "Политика (policy) $\\pi$ — это функция, которая для текущего состояния $s$ дает распределение вероятностей на множестве действий $A$.\n",
        "$$ \\large\n",
        "\\pi(a|s)=\\mathbb{P}\\left[A_{t}=a \\mid S_{t}=s\\right]\n",
        "$$\n",
        "- Политика полностью определяет поведение агента\n",
        "- Политики MDP зависят от текущего состояния среды (а не от прошлых состояний)\n",
        "- т. е. политики являются стационарными (не зависящими от времени),\n",
        "$$ \\large\n",
        "A_{t} \\sim \\pi\\left(\\cdot \\mid S_{t}\\right), \\forall t>0\n",
        "$$\n",
        "\n",
        "Для нашего примера со студентом:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgx_XJvEp0U8"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/EduNet-content/L15/out/markov_policy_example.png\" alt=\"Drawing\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qgAWnOzp0U8"
      },
      "source": [
        "### Value function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOBBL9_Lp0U8"
      },
      "source": [
        "Для оценки политики вводятся функции $V$ (_state-value function_) и $Q$ (_action-value function_):\n",
        "\n",
        "* $v_\\pi(s)$ — измеряет ценность каждого состояния, а именно какое ожидаемое вознаграждение можно получить, если начать двигаться из состояния $s$ в течение всего оставшегося времени, придерживаясь политики $\\pi$.\n",
        "\n",
        "Так, например, агент может следовать политике, заключающейся в переходе между состояниями с высокой ценностью. Однако функция ценности состояния удалена от принятия решений агентом, поскольку агент зачастую не может сам выбрать следующее состояние, в которое перейдет, поскольку на него будет оказываться влияние случайности (например, броски кубиков) или внешних сил (ход оппонента). Поэтому при выборе оптимальной политики рассматривают ценность не состояний, а действий.\n",
        "\n",
        "* $q_\\pi(s,\\color{red}a)$ — аналогично функции ценности состояния, но при условии, что в состоянии $s (s_0 = s)$ было выбрано действие $a$ (мы фиксируем действие). То есть в $q$ мы, в отличие от $v$, фиксируем наше первое действие (не обязательно соответствующее политике), а в $v$ мы первое действие выбираем согласно политике $\\pi$.\n",
        "\n",
        "Матожидание здесь берется по нашей политике, т.к. она стохастична."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY-dgRzGp0U8"
      },
      "source": [
        "### Определение Value Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biEix0Cbp0U8"
      },
      "source": [
        "**State-value function** $v_{\\pi}(s)$ MDP — ожидаемая выгода, начиная с состояния $s$, при условии, что агент следует политике $\\pi$:\n",
        "$$ \\large\n",
        "v_{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s\\right] = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}\\gamma^{k+1}R_{t+k+1} \\mid S_{t}=s\\right]\n",
        "$$\n",
        "**Action-value function** $q_{\\pi}(s, a)$ — ожидаемая выгода, начиная с состояния $s$, действия $a$, и при условии следования политике $\\pi$:\n",
        "$$ \\large\n",
        "q_{\\pi}(s,\\color{red}a)=\\mathbb{E}_{\\pi}\\left[G_{t} \\mid S_{t}=s, \\color{red}{A_{t}=a}\\right] = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}\\gamma^{k+1}R_{t+k+1} \\mid S_{t}=s, \\color{red}{A_{t}=a}\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js1D_UFDzjgR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk6OVWfxz1xD"
      },
      "source": [
        "# Gym\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ66HPy-p0Ub"
      },
      "source": [
        "\n",
        "Давайте разберемся, что такое Gymnasium, и какие возможности он предоставляет для обучения с подкреплением.\n",
        "\n",
        "**Gymnasium** — это набор инструментов для разработки и сравнения алгоритмов RL, позволяющий стандартизировать взаимодействие между разными алгоритмами RL и средами. Также он предоставляет набор стандартных сред, которые могут, в том числе, использоваться для бенчмаркинга.\n",
        "\n",
        "[Gymnasium projects](https://openai.com/projects/)\n",
        "\n",
        "[Документация](https://gymnasium.farama.org/)\n",
        "\n",
        "[Colab demonstration](https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_12_01_ai_gym.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUL2XVQp0Ub"
      },
      "source": [
        "Для начала рассмотрим устройство среды Gym в целом.\n",
        "__Среда__ — это некоторая модель мира, отвечающая за предоставление наблюдений и вознаграждений, в которой существует __агент__, Состояние среды будет изменяться в зависимости от действий агента.\n",
        "\n",
        "Рассмотрим для начала стандартную среду MountainCar, в которой стоит задача — довести машину до вершины горы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-nVYjSWp0Ub"
      },
      "source": [
        "<img src=\"https://edunet.kea.su/repo/EduNet-web_dependencies/L15/mountain-car-v0.gif\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDRa80wIp0Ub",
        "outputId": "81df742b-2181-437a-d9e4-a95fab31ac5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/953.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m747.5/953.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.8/953.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CVictsEp0Uu"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FEhwlrHp0Uv"
      },
      "source": [
        "В Gym среды представлены классом `gym.Env`, который является унифицированным интерфейсом среды со следующими атрибутами и методами:\n",
        "* `action_space`: описание действий, допустимых в данной среде;\n",
        "* `observation_space`: структура и допустимые значения наблюдений состояния среды;\n",
        "* `reset()`: сбрасывает среду и возвращает случайное исходное состояние;\n",
        "* `step(action)`: метод, продвигающий развитие окружающей среды на одно действие и возвращающий информацию о результате этого действия, а именно:\n",
        "    1. observation &mdash; следующее наблюдение;\n",
        "    2. reward &mdash; локальное вознаграждение;\n",
        "    3. done &mdash; флаг конца эпизода.\n",
        "\n",
        "Также в классе `gym.Env` есть несколько вспомогательных методов, например `render()`, позволяющий представить наблюдение в понятной человеку форме, но мы не будем их касаться в нашей лекции."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcvxhMqwp0Uv"
      },
      "source": [
        "### Пространства действий и наблюдений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bprUNAHp0Uv"
      },
      "source": [
        "Действия агента могут быть дискретными, непрерывными и комбинированными. Дискретные действия представлены фиксированным набором и взаимно исключают друг друга, например нажатие/отпускание клавиши. Непрерывным действиям соответствуют значения в некотором диапазоне, например, поворот руля от -720 до 720 градусов. В среде мы не ограничены каким-то одним действием и вполне допустимо одновременное нажатие нескольких кнопок и одновременный поворот руля. Аналогичным образом наблюдения могут быть дискретыми (лампочка включена/выключена) или непрерывными (тензоры, соответствующие цветным изображениям).\n",
        "\n",
        "Давайте посмотрим на то, как выглядят пространства действий и наблюдений в среде MountainCar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_5RMKQPp0Uv",
        "outputId": "1850344c-2b7e-44c1-be88-f5041427a8e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The action space: Discrete(3)\n",
            "The observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
          ]
        }
      ],
      "source": [
        "from warnings import simplefilter\n",
        "\n",
        "simplefilter(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "\n",
        "# Action and observation space\n",
        "action_space = env.action_space\n",
        "obs_space = env.observation_space\n",
        "\n",
        "print(f\"The action space: {action_space}\")\n",
        "print(f\"The observation space: {obs_space}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6djkNcRKp0Uw"
      },
      "source": [
        "Мы видим, что пространство наблюдений и действий представлено некоторыми классами `Box` и `Discrete` соответственно. Что же это за классы? Для объединения нескольких пространств действий (например, непрерывных и дискретных) в одно действие, в Gym существует специальный класс контейнеров.\n",
        "\n",
        "* Класс `Discrete` представляет набор $n$ взаимоисключающих элементов. Например, `Discrete(n=4)` может быть использован для пространства действий с 4 направлениями движения ($\\leftarrow \\downarrow \\rightarrow \\uparrow$)\n",
        "* Класс `Box` представляет n-мерный тензор рациональных чисел в некотором диапазоне `[low, high]`. Например, нажатие педали газа от минимального значения 0 до максимального &mdash; 1, можно закодировать как `с`, аналогично наблюдение экрана игры, можно закодировать как `Box(low=0, high=255, shape=(100, 50, 3), dtype=np.float32)`\n",
        "\n",
        "Давайте посмотрим на диапазон допустимых значений пространства наблюдений в среде MountainCar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YwTzgcop0Uw",
        "outputId": "f00ba923-baee-465d-b8c0-448fbba48c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Upper Bound for Env Observation [0.6  0.07]\n",
            "Lower Bound for Env Observation [-1.2  -0.07]\n"
          ]
        }
      ],
      "source": [
        "print(\"Upper Bound for Env Observation\", env.observation_space.high)\n",
        "print(\"Lower Bound for Env Observation\", env.observation_space.low)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asMLpXfGp0Uw"
      },
      "source": [
        "Обе структуры данных происходят от класса `gym.Space`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o34f9YSpp0Uw",
        "outputId": "05f58b27-bbf4-4840-8675-57d5fbc511f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'gymnasium.spaces.discrete.Discrete'>\n",
            "<class 'gymnasium.spaces.box.Box'>\n"
          ]
        }
      ],
      "source": [
        "print(type(env.action_space))\n",
        "print(type(env.observation_space))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz6wzGYAp0Uw"
      },
      "source": [
        "Также стоит упомянуть еще один дочерний класс  `gym.Space` &mdash; `Tuple`, позволяющий объединять несколько экземпляров класса `gym.Space` вместе. Благодаря этому классу мы можем создавать пространства действий и наблюдений любой сложности.\n",
        "\n",
        "В классе `gym.Space` реализованы методы, позволяющие взаимодействовать с пространствами действий и наблюдений:\n",
        "* `sample()`: возвращает случайный пример из пространства наблюдений\n",
        "* `contains(x)`: проверяет принадлежность аргумента к пространству наблюдений\n",
        "\n",
        "Давайте возьмем случайное действие, доступное нам в исходном состоянии среды MountainCar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGl_Rfx1p0Ux",
        "outputId": "e893c7d7-0ce9-48e5-839e-315721e35be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "random_action = env.action_space.sample()  # random number from 0 to 2\n",
        "print(random_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61eCdwoop0Ux"
      },
      "source": [
        "### Взаимодействие со средой"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVPG5tBxp0Ux"
      },
      "source": [
        "Давайте попробуем совершить случайное действие, которое мы выбрали выше, для этого перезапустим среду, чтобы вернуться в изначальное состояние, и сделаем шаг с помощью метода `step()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lg4v69Pp0Ux",
        "outputId": "21fb32e9-76bc-4dde-be4a-58b99a666892"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The initial observation is [-0.48531637  0.        ]\n",
            "The new observation is [-4.8560286e-01 -2.8648737e-04]\n"
          ]
        }
      ],
      "source": [
        "# reset the environment and see the initial observation\n",
        "obs, info = env.reset()\n",
        "print(f\"The initial observation is {obs}\")\n",
        "\n",
        "# Take the action and get the new observation space\n",
        "new_obs, reward, done, truncated, info = env.step(random_action)\n",
        "print(f\"The new observation is {new_obs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8xlbP9Hy3Qk"
      },
      "source": [
        "truncated (bool) — выполняется ли условие усечения вне области действия MDP. Как правило, это ограничение по времени, но его также можно использовать для обозначения того, что агент физически выходит за границы. Может использоваться для преждевременного завершения эпизода до того, как будет достигнуто терминальное состояние. Если это правда, пользователю необходимо вызвать reset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVSB3EYHzj2g",
        "outputId": "f13d921e-6cbc-4ef9-de8c-a052cfff76fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "truncated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtgYbDNDI3Se"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
