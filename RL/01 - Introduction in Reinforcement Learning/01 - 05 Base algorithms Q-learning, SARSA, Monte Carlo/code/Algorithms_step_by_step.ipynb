{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b42498",
   "metadata": {},
   "source": [
    "# Monte-Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af92220",
   "metadata": {},
   "source": [
    "Основан на оценке значения состояний с помощью среднего взвешенного значения наград, полученных в ходе эпизодов взаимодействия агента со средой. Он относится к методам оценки значения (Value Estimation), которые сосредотачиваются на оценке функции значения и оптимизации на основе этой оценки.\n",
    "\n",
    "Основные шаги метода Монте-Карло:\n",
    "\n",
    "\n",
    "1. Задаем некоторое первое приближение политики $\\pi_0$\n",
    "2. Задаем количество эпизодов моделирования $N$.\n",
    "3. Задаем некоторое текущее приближение политики $\\pi$ (на первом шаге будет равно $\\pi_0)$\n",
    "3. Инициализируем массивы $Q(S, A) = 0$ и $N(S, A) = 0$\n",
    "4. Задаем количество итераций в траектории: $K$ и для каждой итерации (т.е в цикле для каждого $k \\in [1, K]$) выполняем:\n",
    "    - согласно текущей политики $\\pi$ проходим по среде и получаем траекторию $\\tau=(S_0, S_1, \\dots S_T)$, функцию раграды $R_{\\tau} = (R_0, R_1, \\dots, R_{T-1})$ и суммарную награду по траектории $G_{\\tau} = (G_0, G_1, \\dots, G_{T-1}))$.\n",
    "    - возвращаемся в начало траектории и движемся по ней на каждом шаге $t$ обновляя оценку функции $Q(S_t, A_t) \\gets Q(S_t, A_t) + \\frac{1}{N(S_t, A_t)+1}(G_t - Q(S_t, A_t))$. \n",
    "    - проходим в цикле по всем $k$.\n",
    "5. Обновляем политику выбирая для каждого состояния ту аппроксимацию функции $Q(s, a)$ которая дает наибольший прирост суммарной награды и с некоторой вероятностью $\\epsilon$ другую аппроксимацию, что обеспечивает блуждание и исследоание системы на наличие новых состояний.\n",
    "6. Возвращаемся в начало в п.3 в качестве приближения политики используя текущую политику $\\pi$\n",
    "7. Проходим по в цикле по всем эпизодам $n \\in N$\n",
    "\n",
    "Фактически мы получаем внешний цикл по эпизодам и внутри вложенные циклы: цикл по траекториям, потом цикл по расчету награды на каждом шаге и затем цикл по обновлению фнукции $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bde600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarlo(env, episode_n, trajectory_len=500, gamma=0.99):\n",
    "    total_rewards = []  # Создаем список для хранения общих вознаграждений для каждого эпизода\n",
    "    \n",
    "    state_n = env.observation_space.n  # Получаем количество состояний в среде\n",
    "    action_n = env.action_space.n  # Получаем количество действий в среде\n",
    "    qfunction = np.zeros((state_n, action_n))  # Создаем Q-функцию (матрицу состояние-действие) и инициализируем её нулями\n",
    "    counter = np.zeros((state_n, action_n))  # Создаем счетчик для подсчета количества визитов в каждую ячейку Q-функции\n",
    "    \n",
    "    for episode in range(episode_n):  # Запускаем цикл для каждого эпизода\n",
    "        epsilon = 1 - episode / episode_n  # Уменьшаем параметр epsilon для epsilon-жадной стратегии с каждым эпизодом\n",
    "        trajectory = {'states': [], 'actions': [], 'rewards': []}  # Создаем структуру данных для хранения траектории эпизода\n",
    "        \n",
    "        state = env.reset()[0]  # Сбрасываем среду и получаем начальное состояние\n",
    "        for _ in range(trajectory_len):  # Запускаем цикл для каждого шага внутри эпизода (ограниченного trajectory_len)\n",
    "            trajectory['states'].append(state)  # Добавляем текущее состояние в траекторию\n",
    "            \n",
    "            action = get_epsilon_greedy_action(qfunction[state], epsilon, action_n)  # Получаем действие с использованием epsilon-жадной стратегии\n",
    "            trajectory['actions'].append(action)  # Добавляем текущее действие в траекторию\n",
    "            \n",
    "            state, reward, done, _, _ = env.step(action)  # Выполняем выбранное действие и получаем следующее состояние, вознаграждение и флаг завершения\n",
    "            trajectory['rewards'].append(reward)  # Добавляем полученное вознаграждение в траекторию\n",
    "            \n",
    "            if done:  # Если эпизод завершился, выходим из цикла\n",
    "                break\n",
    "                \n",
    "        total_rewards.append(sum(trajectory['rewards']))  # Добавляем суммарное вознаграждение текущего эпизода в список\n",
    "        \n",
    "        real_trajectory_len = len(trajectory['rewards'])  # Определяем реальную длину траектории (может быть меньше trajectory_len, если эпизод завершился раньше)\n",
    "        returns = np.zeros(real_trajectory_len + 1)  # Создаем массив для хранения возвращений (returns) на каждом шаге траектории\n",
    "        \n",
    "        for t in range(real_trajectory_len - 1, -1, -1):  # Запускаем цикл для вычисления возвращений для каждого шага в траектории (в обратном порядке)\n",
    "            returns[t] = trajectory['rewards'][t] + gamma * returns[t + 1]  # Вычисляем возвращение с учетом дисконтирования (gamma)\n",
    "            \n",
    "        for t in range(real_trajectory_len):  # Запускаем цикл для обновления Q-функции на каждом шаге в траектории\n",
    "            state = trajectory['states'][t]  # Получаем текущее состояние\n",
    "            action = trajectory['actions'][t]  # Получаем текущее действие\n",
    "            qfunction[state][action] += (returns[t] - qfunction[state][action]) / (1 + counter[state][action])  # Обновляем Q-функцию согласно формуле метода Монте-Карло\n",
    "            counter[state][action] += 1  # Увеличиваем счетчик визитов для соответствующей ячейки Q-функции\n",
    "            \n",
    "    return total_rewards  # Возвращаем список общих вознаграждений для каждого эпизода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edddb0",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab4a2f",
   "metadata": {},
   "source": [
    "Основные шаги метода SARSA:\n",
    "\n",
    "1. Устанавливаем количество эпизодов $N$.\n",
    "2. Внутри эпизода начинаем проходить по шагам траектории $t$ и на каждом шаге:\n",
    "    - Выполняем выбранное действие и получаем следующее состояние, вознаграждение и флаг завершения\n",
    "    - Получаем следующее действие с использованием epsilon-жадной стратегии\n",
    "    - Обновляем Q-функцию с использованием следующего состояния и следующего действия __согласно политики__ (отличие от алгоритма q-learning)\n",
    "    - Добавляем полученное вознаграждение к общему вознаграждению текущего эпизода\n",
    "3. Завершаем эпизод, обновляем $\\epsilon$ и переходим к следующему эпизоду.\n",
    "\n",
    "$\\alpha$ - шиперпараметр имеющий смысл скорости обучения. $\\gamma$ - гиперпараметр имеющий смысл коэффициента дисконтирования, т.е определяющего что более ценно награда сейчас или награда будущих шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8f29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(env, episode_n, gamma=0.99, trajectory_len=500, alpha=0.5):\n",
    "    total_rewards = np.zeros(episode_n)  # Создаем массив для хранения общих вознаграждений для каждого эпизода\n",
    "    \n",
    "    state_n = env.observation_space.n  # Получаем количество состояний в среде\n",
    "    action_n = env.action_space.n  # Получаем количество действий в среде\n",
    "    qfunction = np.zeros((state_n, action_n))  # Создаем Q-функцию (матрицу состояние-действие) и инициализируем её нулями\n",
    "    \n",
    "    for episode in range(episode_n):  # Запускаем цикл для каждого эпизода\n",
    "        epsilon = 1 / (episode + 1)  # Уменьшаем параметр epsilon для epsilon-жадной стратегии с каждым эпизодом\n",
    "        \n",
    "        state = env.reset()[0]  # Сбрасываем среду и получаем начальное состояние\n",
    "        action = get_epsilon_greedy_action(qfunction[state], epsilon, action_n)  # Получаем действие с использованием epsilon-жадной стратегии\n",
    "        \n",
    "        for _ in range(trajectory_len):  # Запускаем цикл для каждого шага внутри эпизода (ограниченного trajectory_len)\n",
    "            next_state, reward, done, _, _ = env.step(action)  # Выполняем выбранное действие и получаем следующее состояние, вознаграждение и флаг завершения\n",
    "            next_action = get_epsilon_greedy_action(qfunction[next_state], epsilon, action_n)  # Получаем следующее действие с использованием epsilon-жадной стратегии\n",
    "            \n",
    "            qfunction[state][action] += alpha * (reward + gamma * qfunction[next_state][next_action] - qfunction[state][action])  # Обновляем Q-функцию согласно формуле метода SARSA\n",
    "            \n",
    "            state = next_state  # Переходим в следующее состояние\n",
    "            action = next_action  # Переходим в следующее действие\n",
    "            \n",
    "            total_rewards[episode] += reward  # Добавляем полученное вознаграждение к общему вознаграждению текущего эпизода\n",
    "            \n",
    "            if done:  # Если эпизод завершился, выходим из цикла\n",
    "                break\n",
    "\n",
    "    return total_rewards  # Возвращаем массив общих вознаграждений для каждого эпизода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7339786",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ea301",
   "metadata": {},
   "source": [
    "Основные шаги метода Q-learning:\n",
    "\n",
    "1. Устанавливаем количество эпизодов $N$.\n",
    "2. Внутри эпизода начинаем проходить по шагам траектории $t$ и на каждом шаге:\n",
    "    - Выполняем выбранное действие и получаем следующее состояние, вознаграждение и флаг завершения\n",
    "    - Получаем следующее действие с использованием epsilon-жадной стратегии\n",
    "    - Обновляем Q-функцию с использованием следующего состояния и __максимального__ Q-значения для следующего состояния (отличие от алгоритма SARSA). Т.е берем строку со всеми возможными значениями Q функции в следующем состоянии, т.е от всех возможных\n",
    "    - Добавляем полученное вознаграждение к общему вознаграждению текущего эпизода\n",
    "3. Завершаем эпизод, обновляем $\\epsilon$ и переходим к следующему эпизоду.\n",
    "\n",
    "$\\alpha$ - шиперпараметр имеющий смысл скорости обучения. $\\gamma$ - гиперпараметр имеющий смысл коэффициента дисконтирования, т.е определяющего что более ценно награда сейчас или награда будущих шагов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(env, episode_n, noisy_episode_n, gamma=0.99, t_max=500, alpha=0.5):\n",
    "    state_n = env.observation_space.n  # Получаем количество состояний в среде\n",
    "    action_n = env.action_space.n  # Получаем количество действий в среде\n",
    "\n",
    "    Q = np.zeros((state_n, action_n))  # Создаем Q-таблицу размерности (количество состояний) x (количество действий) и инициализируем нулями\n",
    "    epsilon = 1  # Значение параметра epsilon для epsilon-greedy стратегии, которое будет уменьшаться по мере обучения\n",
    "\n",
    "    total_rewards = []  # Список для хранения наград в каждом эпизоде\n",
    "    for episode in range(episode_n):  # Начинаем обучение на заданное количество эпизодов\n",
    "\n",
    "        total_reward = 0  # Суммарная награда в текущем эпизоде\n",
    "        state = env.reset()  # Сбрасываем среду в начальное состояние и получаем начальное состояние\n",
    "\n",
    "        for t in range(t_max):  # В каждом эпизоде продолжаем обучение в течение максимального количества временных шагов t_max\n",
    "            action = get_epsilon_greedy_action(Q[state], epsilon, action_n)  # Получаем действие с использованием epsilon-greedy стратегии на основе текущей Q-таблицы\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)  # Выполняем выбранное действие и получаем следующее состояние, вознаграждение и флаг завершения\n",
    "\n",
    "            Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])  # Обновляем значение Q-таблицы для текущего состояния и выбранного действия на основе полученной награды и максимального Q-значения для следующего состояния\n",
    "\n",
    "            total_reward += reward  # Увеличиваем суммарную награду для текущего эпизода\n",
    "\n",
    "            if done:  # Если агент достиг целевого состояния (done=True), заканчиваем эпизод\n",
    "                break\n",
    "\n",
    "            state = next_state  # Переходим в следующее состояние для следующего временного шага\n",
    "\n",
    "        epsilon = max(0, epsilon - 1 / noisy_episode_n)  # Уменьшаем epsilon с течением времени для уменьшения исследования в пользу использования текущей стратегии\n",
    "\n",
    "        total_rewards.append(total_reward)  # Добавляем суммарную награду текущего эпизода в список total_rewards\n",
    "\n",
    "    return total_rewards  # Возвращаем список суммарных наград для каждого эпизода обучения\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
