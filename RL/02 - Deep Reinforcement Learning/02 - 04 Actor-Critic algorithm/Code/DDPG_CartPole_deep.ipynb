{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recent-bunch",
   "metadata": {},
   "source": [
    "# DDPG (Deep Deterministic Policy Gradient)\n",
    "\n",
    "Until now, we applied policy gradient (on-policy) in order to handle continuos spaces. (This method then leads to modern PPO algorithm)<br>\n",
    "\n",
    "DDPG (Deep Deterministic Policy Gradient) also apply policy gradient and use Actor-Critic architecture, but it's \"deterministic\" and it's then called **off-policy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6122c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1 --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3558de16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa81765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126dd724",
   "metadata": {},
   "source": [
    "Because DDPG cannot operate discrete action space, and here we then use custom CartPole implementation for continuous action space.<br>\n",
    "The shape of action space in this example is ```Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f3b3261-565d-43f0-a3cf-5f046d77e22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.cartpole import CartPole\n",
    "env = CartPole()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a1a1e",
   "metadata": {},
   "source": [
    "First of all, we will start with the following equation (Bellman equation) discussed in [Q-Learning](./01-q-learning.ipynb) :\n",
    "\n",
    "$$ Q^{*}(s_t,a_t) = r_t + \\gamma \\max_a{Q(s_{t+1},a)} \\;\\;\\;\\;\\;\\; (1)$$\n",
    "\n",
    "where $ Q^{*} $ means the optimal $ Q $ value.\n",
    "\n",
    "Now we consider Q-network $ Q_{\\phi} $ where $\\phi$ is parameters.<br>\n",
    "To optimize the equation (1), we should find $\\phi$ to minimize the following loss $L$ for tuples $ (s_t, a_t, r_t, s_{t+1}, d_t) $.\n",
    "\n",
    "$$ L = E\\left[ \\left( Q_{\\phi}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\max_a{Q_{\\phi}(s_{t+1},a)} \\right) \\right)^2 \\right] \\;\\;\\;\\;\\;\\; (2)$$\n",
    "\n",
    "where $d_t = 1$ if the episode is done, and $0$ otherwise.\n",
    "\n",
    "To make the above maximization $\\max_a Q_{\\phi}(s_{t+1}, a)$ work in continuous action space, we introduce the action policy $\\mu_{\\theta}(s)$, with which we'll find $\\theta$ to maximize $ Q_{\\phi}(s, \\mu_{\\theta}(s)) $.<br>\n",
    "The equation (2) will then be written as follows.\n",
    "\n",
    "$$ L = E\\left[ \\left( Q_{\\phi}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) Q_{\\phi}(s_{t+1},\\mu_{\\theta}(s_{t+1})) \\right) \\right)^2 \\right] \\;\\;\\;\\;\\;\\; (3) $$\n",
    "\n",
    "> Note : As you will find, this will again be based on [Actor-Critic method](./03-actor-critic.ipynb) which separates policy ($\\mu_{\\theta}$) and value evaluation ($Q_{\\phi}$) in algorithm.\n",
    "\n",
    "Please take care for the difference between this policy $\\mu_{\\theta}(s)$ and a policy used in [on-policy](./02-policy-gradient.ipynb) method. Here $\\mu_{\\theta}(s)$ is deterministic and it doesn't care how the action space is distributed, unlike $ P(a | \\pi_\\theta (s)) $ in on-policy methods. (This is because DDPG is called \"deterministic\" and it's in off-policy method.)\n",
    "\n",
    "To make the minimization (3) stable, we also introduce the separated parameters $\\phi^{\\prime}$ and $\\theta^{\\prime}$ (called **target**) as follows.\n",
    "\n",
    "$$ L = E\\left[ \\left( Q_{\\phi}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) Q_{{\\phi}^{\\prime}}(s_{t+1},\\mu_{\\theta^{\\prime}}(s_{t+1})) \\right) \\right)^2 \\right] \\;\\;\\;\\;\\;\\; (4) $$\n",
    "\n",
    "And these parameters are delayed with coefficient parameter (hyper-parameter) $ \\tau $ (which is near 0) after each training.\n",
    "\n",
    "$ \\phi^{\\prime} = \\tau \\phi + (1-\\tau) \\phi^{\\prime} $\n",
    "\n",
    "$ \\theta^{\\prime} = \\tau \\theta + (1-\\tau) \\theta^{\\prime} $\n",
    "\n",
    "To summarize algorithm :\n",
    "\n",
    "- Collect tuples $ (s_t, a_t, r_t, s_{t+1}, d_t) $\n",
    "- Optimize $ \\phi $ to minimize $L$ in (4)\n",
    "- Optimize $ \\theta $ to maximize $Q_{\\phi}(s, \\mu_{\\theta}(s))$\n",
    "- Update target parameters as follows\n",
    "    - $ \\phi^{\\prime} = \\tau \\phi + (1-\\tau) \\phi^{\\prime} $\n",
    "    - $ \\theta^{\\prime} = \\tau \\theta + (1-\\tau) \\theta^{\\prime} $\n",
    "\n",
    "Now let's start building networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e178f",
   "metadata": {},
   "source": [
    "## 1. Q-network\n",
    "\n",
    "First, we build Q-network (both $Q_{\\phi}(s, a)$ and $Q_{{\\phi}^{\\prime}}(s, a)$) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f70ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Linear(5, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        outs = torch.concat((s, a), dim=-1)\n",
    "        outs = self.input(outs)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.hidden(outs)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.output(outs)\n",
    "        return outs\n",
    "\n",
    "q_origin_model = QNet().to(device)  # Q_phi\n",
    "q_target_model = QNet().to(device)  # Q_phi'\n",
    "_ = q_target_model.requires_grad_(False)  # target model doen't need grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d556d",
   "metadata": {},
   "source": [
    "## 2. Policy network\n",
    "\n",
    "Next we build policy network (both $\\mu_{\\theta}(s)$ and $\\mu_{\\theta^{\\prime}}(s)$) as follows.\n",
    "\n",
    "As I have mentioned above, action space is ```Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)``` and it's then activated by ```tanh()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5743a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input = nn.Linear(4, hidden_dim)\n",
    "        self.hidden = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, s):\n",
    "        outs = self.input(s)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.hidden(s)\n",
    "        outs = F.relu(outs)\n",
    "        outs = self.output(outs)\n",
    "        outs = torch.tanh(outs)\n",
    "        return outs\n",
    "\n",
    "mu_origin_model = PolicyNet().to(device)  # mu_theta\n",
    "mu_target_model = PolicyNet().to(device)  # mu_theta'\n",
    "_ = mu_target_model.requires_grad_(False)  # target model doen't need grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fb21a",
   "metadata": {},
   "source": [
    "## 3. Function to optimize network parameters $\\phi, \\theta$\n",
    "\n",
    "As I have mentioned above, we optimize parameters as follows :\n",
    "\n",
    "- Optimize $ \\phi $ to minimize $L$ in (4)\n",
    "- Optimize $ \\theta $ to maximize $Q_{\\phi}(s, \\mu_{\\theta}(s))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19bd68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "opt_q = torch.optim.AdamW(q_origin_model.parameters(), lr=0.0005)\n",
    "opt_mu = torch.optim.AdamW(mu_origin_model.parameters(), lr=0.0005)\n",
    "\n",
    "def optimize(states, actions, rewards, next_states, dones):\n",
    "    # Convert to tensor\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "    actions = actions.unsqueeze(dim=1)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n",
    "    rewards = rewards.unsqueeze(dim=1)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float).to(device)\n",
    "    dones = dones.unsqueeze(dim=1)\n",
    "\n",
    "    # Optimize critic loss\n",
    "    opt_q.zero_grad()\n",
    "    q_org = q_origin_model(states, actions)\n",
    "    mu_tgt_next = mu_target_model(next_states)\n",
    "    q_tgt_next = q_target_model(next_states, mu_tgt_next)\n",
    "    q_tgt = rewards + gamma * (1.0 - dones) * q_tgt_next\n",
    "    loss_q = F.mse_loss(\n",
    "        q_org,\n",
    "        q_tgt,\n",
    "        reduction=\"none\")\n",
    "    loss_q.sum().backward()\n",
    "    opt_q.step()\n",
    "\n",
    "    # Optimize actor loss\n",
    "    opt_mu.zero_grad()\n",
    "    mu_org = mu_origin_model(states)\n",
    "    for p in q_origin_model.parameters():\n",
    "        p.requires_grad = False # disable grad in q_origin_model before computation\n",
    "    q_tgt_max = q_origin_model(states, mu_org)\n",
    "    (-q_tgt_max).sum().backward()\n",
    "    opt_mu.step()\n",
    "    for p in q_origin_model.parameters():\n",
    "        p.requires_grad = True # enable grad again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8100e",
   "metadata": {},
   "source": [
    "## 4. Function to update target parameters $\\phi^{\\prime}, \\theta^{\\prime}$\n",
    "\n",
    "Target parameters are updated as follows.\n",
    "\n",
    "- $ \\phi^{\\prime} = \\tau \\phi + (1-\\tau) \\phi^{\\prime} $\n",
    "- $ \\theta^{\\prime} = \\tau \\theta + (1-\\tau) \\theta^{\\prime} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10bb910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.002\n",
    "\n",
    "def update_target():\n",
    "    for var, var_target in zip(q_origin_model.parameters(), q_target_model.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n",
    "    for var, var_target in zip(mu_origin_model.parameters(), mu_target_model.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc1f7f1",
   "metadata": {},
   "source": [
    "## 5. Replay buffer\n",
    "\n",
    "In on-policy architecture, we fed the sequential samples (trajectory) for each training batch.<br>\n",
    "To prevent from learning only for recent experiences in DDPG, we store the past experience of tuples $ (s_t, a_t, r_t, s_{t+1}, d_t) $ in buffer (called \"replay buffer\") and pick up the randomized samples in batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1172351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class replayBuffer:\n",
    "    def __init__(self, buffer_size: int):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def add(self, item):\n",
    "        if len(self.buffer) > self._next_idx:\n",
    "            self.buffer[self._next_idx] = item\n",
    "        else:\n",
    "            self.buffer.append(item)\n",
    "        if self._next_idx == self.buffer_size - 1:\n",
    "            self._next_idx = 0\n",
    "        else:\n",
    "            self._next_idx = self._next_idx + 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = [random.randint(0, len(self.buffer) - 1) for _ in range(batch_size)]\n",
    "        states   = [self.buffer[i][0] for i in indices]\n",
    "        actions  = [self.buffer[i][1] for i in indices]\n",
    "        rewards  = [self.buffer[i][2] for i in indices]\n",
    "        n_states = [self.buffer[i][3] for i in indices]\n",
    "        dones    = [self.buffer[i][4] for i in indices]\n",
    "        return states, actions, rewards, n_states, dones\n",
    "\n",
    "    def length(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "buffer = replayBuffer(buffer_size=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4292f941",
   "metadata": {},
   "source": [
    "## 6. Noise for exploration\n",
    "\n",
    "As I mentioned above, the policy $\\mu_{\\theta}(s)$ is deterministic and it doesn't care how the actions are selected. In order for this reason, the noise will be added for exploration to pick up samples in episode's trials. (If not, this will stuck in the same values forever.)<br>\n",
    "In DDPG, the following noise (so called [Ornstein-Uhlenbeck](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) noise) is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e192851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ornstein-Uhlenbeck noise implemented by OpenAI\n",
    "Copied from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py\n",
    "\"\"\"\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "ou_action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(1), sigma=np.ones(1) * 0.05)\n",
    "\n",
    "# pick up action with Ornstein-Uhlenbeck noise\n",
    "def pick_sample(s):\n",
    "    with torch.no_grad():\n",
    "        s = np.array(s)\n",
    "        s_batch = np.expand_dims(s, axis=0)\n",
    "        s_batch = torch.tensor(s_batch, dtype=torch.float).to(device)\n",
    "        action_det = mu_origin_model(s_batch)\n",
    "        action_det = action_det.squeeze(dim=1)\n",
    "        noise = ou_action_noise()\n",
    "        action = action_det.cpu().numpy() + noise\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        return float(action.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b19a01",
   "metadata": {},
   "source": [
    "(7) Now let's put it all together !\n",
    "\n",
    "I note that here I train (optimize parameters) in each step, and it might then be slow to step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe030199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 1503/2000 [03:11<01:30,  5.52it/s]"
     ]
    }
   ],
   "source": [
    "batch_size = 250\n",
    "\n",
    "reward_records = []\n",
    "for i in tqdm(range(2000)):\n",
    "    # Run episode till done\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "    while not done:\n",
    "        a = pick_sample(s)\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        buffer.add([s, a, r, s_next, float(done)])\n",
    "        cum_reward += r\n",
    "\n",
    "        # Train (optimize parameters)\n",
    "        if buffer.length() >= batch_size:\n",
    "            states, actions, rewards, n_states, dones = buffer.sample(batch_size)\n",
    "            optimize(states, actions, rewards, n_states, dones)\n",
    "            update_target()\n",
    "        s = s_next\n",
    "\n",
    "    # Output total rewards in episode (max 500)\n",
    "    # print(\"Run episode{} with rewards {}\".format(i, cum_reward), end=\"\\r\")\n",
    "    reward_records.append(cum_reward)\n",
    "\n",
    "    # stop if reward mean > 475.0\n",
    "    if np.average(reward_records[-50:]) > 475.0:\n",
    "        break\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70d022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Generate recent 50 interval average\n",
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "plt.plot(reward_records, label='reward')\n",
    "plt.plot(average_reward, label='average reward')\n",
    "plt.xlabel('N steps')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f3f20",
   "metadata": {},
   "source": [
    "## [Optional] Clipped double-Q learning (Twin-Q)\n",
    "\n",
    "In advanced algorithms, such as TD3 (Twin Delayed DDPG), 2 Q-networks - $ Q_{\\phi_1}(s, a), Q_{\\phi_2}(s, a) $ - and corresponding 2 target networks - $ Q_{\\phi_1^{\\prime}}(s, a), Q_{\\phi_2^{\\prime}}(s, a) $ - are used to optimize in order to mitigate the risk of overestimation in Q-function.\n",
    "\n",
    "In this double-Q algorithms, we optimize parameters as follows :\n",
    "\n",
    "- Optimize $ \\phi_1 $ to minimize $ E\\left[ \\left( Q_{\\phi_1}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\min_{i=1,2} Q_{{\\phi_i}^{\\prime}}(s_{t+1},\\mu_{\\theta^{\\prime}}(s_{t+1})) \\right) \\right)^2 \\right] $\n",
    "- Optimize $ \\phi_2 $ to minimize $ E\\left[ \\left( Q_{\\phi_2}(s_t, a_t) - \\left( r_t + \\gamma (1 - d_t) \\min_{i=1,2} Q_{{\\phi_i}^{\\prime}}(s_{t+1},\\mu_{\\theta^{\\prime}}(s_{t+1})) \\right) \\right)^2 \\right] $\n",
    "- Optimize $ \\theta $ to maximize $Q_{\\phi_1}(s, \\mu_{\\theta}(s))$\n",
    "\n",
    "And update target parameters as follows :\n",
    "\n",
    "- $ \\phi_1^{\\prime} = \\tau \\phi_1 + (1-\\tau) \\phi_1^{\\prime} $\n",
    "- $ \\phi_2^{\\prime} = \\tau \\phi_2 + (1-\\tau) \\phi_2^{\\prime} $\n",
    "- $ \\theta^{\\prime} = \\tau \\theta + (1-\\tau) \\theta^{\\prime} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36d19c",
   "metadata": {},
   "source": [
    "(1) First, regenerate Q-network and policy network as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_origin_model1 = QNet().to(device)  # Q_phi1\n",
    "q_origin_model2 = QNet().to(device)  # Q_phi2\n",
    "q_target_model1 = QNet().to(device)  # Q_phi1'\n",
    "q_target_model2 = QNet().to(device)  # Q_phi2'\n",
    "_ = q_target_model1.requires_grad_(False)  # target model doen't need grad\n",
    "_ = q_target_model2.requires_grad_(False)  # target model doen't need grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47433b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_origin_model = PolicyNet().to(device)  # mu_theta\n",
    "mu_target_model = PolicyNet().to(device)  # mu_theta'\n",
    "_ = mu_target_model.requires_grad_(False)  # target model doen't need grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6336f2",
   "metadata": {},
   "source": [
    "(2) Regenerate optimize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c153b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "opt_q1 = torch.optim.AdamW(q_origin_model1.parameters(), lr=0.0005)\n",
    "opt_q2 = torch.optim.AdamW(q_origin_model2.parameters(), lr=0.0005)\n",
    "opt_mu = torch.optim.AdamW(mu_origin_model.parameters(), lr=0.0005)\n",
    "\n",
    "def optimize(states, actions, rewards, next_states, dones):\n",
    "    # Convert to tensor\n",
    "    states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "    actions = torch.tensor(actions, dtype=torch.float).to(device)\n",
    "    actions = actions.unsqueeze(dim=1)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float).to(device)\n",
    "    rewards = rewards.unsqueeze(dim=1)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float).to(device)\n",
    "    dones = torch.tensor(dones, dtype=torch.float).to(device)\n",
    "    dones = dones.unsqueeze(dim=1)\n",
    "\n",
    "    # Compute r + gamma * (1 - done) * min Q (s_next, mu_target(s_next))\n",
    "    mu_tgt_next = mu_target_model(next_states)\n",
    "    q1_tgt_next = q_target_model1(next_states, mu_tgt_next)\n",
    "    q2_tgt_next = q_target_model2(next_states, mu_tgt_next)\n",
    "    q_tgt_next_min = torch.minimum(q1_tgt_next, q2_tgt_next)\n",
    "    q_tgt = rewards + gamma * (1.0 - dones) * q_tgt_next_min\n",
    "\n",
    "    # Optimize critic loss for Q-network1\n",
    "    opt_q1.zero_grad()\n",
    "    q1_org = q_origin_model1(states, actions)\n",
    "    loss_q1 = F.mse_loss(\n",
    "        q1_org,\n",
    "        q_tgt,\n",
    "        reduction=\"none\")\n",
    "    loss_q1.sum().backward()\n",
    "    opt_q1.step()\n",
    "\n",
    "    # Optimize critic loss for Q-network2\n",
    "    opt_q2.zero_grad()\n",
    "    q2_org = q_origin_model2(states, actions)\n",
    "    loss_q2 = F.mse_loss(\n",
    "        q2_org,\n",
    "        q_tgt,\n",
    "        reduction=\"none\")\n",
    "    loss_q2.sum().backward()\n",
    "    opt_q2.step()\n",
    "\n",
    "    # Optimize actor loss\n",
    "    opt_mu.zero_grad()\n",
    "    mu_org = mu_origin_model(states)\n",
    "    for p in q_origin_model1.parameters():\n",
    "        p.requires_grad = False # disable grad in q_origin_model1 before computation\n",
    "    q_tgt_max = q_origin_model1(states, mu_org)\n",
    "    (-q_tgt_max).sum().backward()\n",
    "    opt_mu.step()\n",
    "    for p in q_origin_model1.parameters():\n",
    "        p.requires_grad = True # enable grad again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b612e",
   "metadata": {},
   "source": [
    "(3) Regenerate update target's params function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c9418",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.002\n",
    "\n",
    "def update_target():\n",
    "    for var, var_target in zip(q_origin_model1.parameters(), q_target_model1.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n",
    "    for var, var_target in zip(q_origin_model2.parameters(), q_target_model2.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data\n",
    "    for var, var_target in zip(mu_origin_model.parameters(), mu_target_model.parameters()):\n",
    "        var_target.data = tau * var.data + (1.0 - tau) * var_target.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a631fe",
   "metadata": {},
   "source": [
    "(4) Let's start training again !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset noise\n",
    "ou_action_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(1), sigma=np.ones(1) * 0.05)\n",
    "\n",
    "# reset buffer\n",
    "buffer = replayBuffer(buffer_size=50000)\n",
    "\n",
    "# start training\n",
    "batch_size = 250\n",
    "reward_records = []\n",
    "for i in range(5000):\n",
    "    # Run episode till done\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "    while not done:\n",
    "        a = pick_sample(s)\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        buffer.add([s, a, r, s_next, float(done)])\n",
    "        cum_reward += r\n",
    "\n",
    "        # Train (optimize parameters)\n",
    "        if buffer.length() >= batch_size:\n",
    "            states, actions, rewards, n_states, dones = buffer.sample(batch_size)\n",
    "            optimize(states, actions, rewards, n_states, dones)\n",
    "            update_target()\n",
    "        s = s_next\n",
    "\n",
    "    # Output total rewards in episode (max 500)\n",
    "    print(\"Run episode{} with rewards {}\".format(i, cum_reward), end=\"\\r\")\n",
    "    reward_records.append(cum_reward)\n",
    "\n",
    "    # stop if reward mean > 475.0\n",
    "    if np.average(reward_records[-50:]) > 475.0:\n",
    "        break\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Generate recent 50 interval average\n",
    "average_reward = []\n",
    "for idx in range(len(reward_records)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = reward_records[:idx+1]\n",
    "    else:\n",
    "        avg_list = reward_records[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "plt.plot(reward_records, label='reward')\n",
    "plt.plot(average_reward, label='average reward')\n",
    "plt.xlabel('N steps')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938533a-6867-4d48-8f9c-67a89765701e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
