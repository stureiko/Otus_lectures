\chapter{Основные алгоритмы}
\section{Основные определения}

\subsection{Марковский процесс принятия решений}
Марковский процесс принятия решений (Markov Decision Process, MDP) - это математическая модель, используемая в теории управления, искусственном интеллекте и обучении с подкреплением для моделирования ситуаций, в которых агент должен принимать решения в неопределенной среде. MDP предоставляет структуру для формализации проблем принятия решений и исследования оптимальных стратегий.

Основные компоненты MDP включают:

1. Состояния (States): Состояния представляют текущее положение или конфигурацию среды, в которой находится агент. Состояния могут быть дискретными или непрерывными, в зависимости от задачи.

2. Действия (Actions): Действия - это множество возможных действий, которые агент может выполнять в каждом состоянии. Действия также могут быть дискретными или непрерывными.

3. Функция переходов (Transition Function): Эта функция описывает вероятность перехода из одного состояния в другое после выполнения определенного действия. Она может быть представлена как $P(s'|s, a)$, где $s$ - текущее состояние, $a$ - действие, $s'$ - следующее состояние.

4. Функция вознаграждения (Reward Function): Функция вознаграждения определяет мгновенное вознаграждение, получаемое агентом при выполнении определенного действия в определенном состоянии. Обычно обозначается как $R(s, a, s')$.

5. Функция ценности (Value Function): Функция ценности определяет ожидаемую награду, которую агент может ожидать получить, начиная с определенного состояния и следуя определенной стратегии. Это позволяет агенту оценивать, насколько хорошо он справляется с задачей в различных состояниях.

6. Политика (Policy): Политика - это правило, которое определяет, какие действия агент должен предпринимать в каждом состоянии. Она может быть стохастической (вероятностной) или детерминированной.

Цель в MDP состоит в том, чтобы найти оптимальную стратегию, которая максимизирует ожидаемую сумму вознаграждений, получаемых агентом в процессе взаимодействия со средой. Это может быть достигнуто с использованием различных алгоритмов обучения с подкреплением, таких как алгоритмы Q-обучения или методы глубокого обучения (DRL).


\subsection{Политика}
В контексте обучения с подкреплением (Reinforcement Learning, RL), "политика" (Policy) представляет собой стратегию или правила, которые определяют, какой выбор действий агент должен сделать в каждом возможном состоянии среды. Политика является центральной концепцией в RL и определяет стратегию агента для достижения его целей в задаче.

Политика может быть стохастической (вероятностной) или детерминированной:

1. Стохастическая политика: В этом случае политика определяет вероятностное распределение над действиями в каждом состоянии. Стохастическая политика полезна, когда агенту необходимо учитывать случайность в среде или в самом агенте.

2. Детерминированная политика: Детерминированная политика просто указывает одно конкретное действие в каждом состоянии. Это означает, что для данного состояния агент всегда выбирает одно и то же действие.

Цель обучения с подкреплением заключается в нахождении оптимальной политики, которая максимизирует сумму наград, получаемых агентом в процессе взаимодействия со средой. Стратегии агента могут изменяться в зависимости от различных факторов, таких как состояние среды, его опыт и обновленные оценки ценности.

Существует несколько методов для нахождения оптимальной политики, включая методы, основанные на оценке функций ценности (например, методы Q-обучения) и методы, использующие прямую оптимизацию политики (например, метод глубокого обучения с подкреплением, такие как DDPG или PPO).

Политика - это ключевой элемент в RL, поскольку она определяет, как агент взаимодействует со средой и каким образом он выбирает действия для достижения своих целей.

В уравнениях политика как правило обозначается как $\pi(a|s)$, т.е. показывает какое действие $a$ будет выполнено из состояния $s$ согласно текущей политики $\pi$.

\subsection{$v$-функция и $q$-функция}
Функция ценности состояния (State Value Function, $v$-функция, $V(s)$): Эта функция оценивает ожидаемую награду, которую агент может ожидать получить, начиная из конкретного состояния и следуя определенной стратегии. Функция $V(s)$ показывает, насколько ценно находиться в состоянии s, если агент действует согласно текущей стратегии. В математической форме, $V(s)$ представляет собой среднюю сумму будущих вознаграждений, взятых с учетом вероятностей переходов и вознаграждений.

Так же оценку $v$-функции при текущей политике $\pi$ будем обозначать $v_{\pi}(s)$.


Функция ценности действия (Action Value Function, $q$-функция, $Q(s, a)$): Эта функция оценивает ожидаемую награду, которую агент может ожидать получить, начиная из состояния s, выполняя действие a, а затем следуя определенной стратегии. Она измеряет ценность выполнения конкретного действия в конкретном состоянии при следовании текущей стратегии.

\subsubsection{Связь между $v$-функцией и $q$-функцией}
По определению мы можем увидеть связь между $v$-функцией и $q$-функцией:
$$ V(s) = \sum_{a}\pi(a|s)Q(s,a) $$

\subsection{Уравнение Беллмана}

Уравнение Беллмана для $v$-функции описывает связь между оценкой значения состояния ($v$-функции) и ожидаемыми наградами в текущем состоянии и всех возможных следующих состояниях.

Это уравнение формализует принцип оптимальности и позволяет находить оптимальные оценки значений состояний.

$$v_{\pi}(s)=\sum_{a}\pi(a|s)\Big( R(s, a) + \gamma \sum_{a'}P(s'|s, a)v_{\pi}(s') \Big)$$

Здесь:

$v(s)$ - значение $v$-функции в состоянии $s$ , т.е. ожидаемая сумма вознаграждений,
которую агент может получить, находясь в состоянии $s$.

$\pi(a|s)$ - вероятность выбора действия $a$ в состоянии $s$ при использовании стратегии $\pi$.

$P(s'|s, a)$ - вероятность перехода в состояние $s'$ и получения вознаграждения $R(s,a)$ при выполнении действия $a$ в состоянии $s$.

$\gamma$ - коэффициент дисконтирования, который определяет важность будущих вознаграждений по сравнению с немедленными вознаграждениями.


Уравнение Беллмана для $q$-функции описывает связь между оценкой значения действия из текущего состояния ($q$-функции) и ожидаемыми наградами в текущем состоянии и всех возможных следующих состояниях при всех возможных действиях.

$$q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s'}P(s'|s, a)\sum_{a'}\pi(a'|s')q_{\pi}(s', a')$$


\subsection{Эпсилон-жадная политика}

Эпсилон-жадная политика (Epsilon-Greedy Policy) - это стратегия принятия решений , которая объединяет жадность (greedy) и исследование (exploration) с целью обеспечения баланса между выбором наилучших известных действий и исследованием других альтернатив.

Суть эпсилон-жадной политики заключается в следующем:

1. Агент выбирает наилучшее действие с вероятностью 1 - $\varepsilon$, где $\varepsilon$ (эпсилон) - это небольшое положительное число, обычно в интервале [0, 1], которое определяет уровень исследования агента.

2. Агент выбирает случайное действие с вероятностью $\varepsilon$. Это действие выбирается равномерно случайным образом из всех доступных действий. Это обеспечивает исследование новых стратегий и состояний среды.

По сути, эпсилон-жадная политика позволяет агенту эксплуатировать известные хорошие действия (жадность), чтобы максимизировать награду, и в то же время исследовать неизведанные действия с некоторой вероятностью (исследование), чтобы обновить свои оценки и учиться лучшим стратегиям.

Параметр $\varepsilon$ является ключевым фактором в эпсилон-жадной политике. Выбор оптимального значения $\varepsilon$ зависит от конкретной задачи и среды. Если $\varepsilon$ очень маленькое (близкое к нулю), то агент будет больше склоняться к жадной стратегии и меньше исследовать. Если $\varepsilon$ близко к единице, агент будет более склонен к случайному исследованию, нежели к эксплуатации известных действий. Выбор оптимального значения $\varepsilon$ зависит от баланса между необходимостью исследования и максимизации награды в задаче обучения с подкреплением.

Так же часто параметр $\varepsilon$ уменьшают по мере обучения агента.

Формально запишем $\varepsilon$-жадную политику следующим образом:
$$
\pi(a|s) = 
 \begin{cases}
	1 - \varepsilon + \frac{\varepsilon}{m},& \underset{a'}{\mathrm{arg max}}\, Q(s,a')\\
	\frac{\varepsilon}{m}
\end{cases}
$$

\section{Cross-entropy method}
Алгоритм кросс-энтропии относится к классу алгоритмов динамического программирования.

Данный алгоритм применим при следующих ограничениях:
\begin{itemize}
	\item конечное пространство состояний;
	\item конечное пространство действий;
	\item детерминированная функция перехода - т.е по текущему состоянию и текущему действию \textbf{однозначно} определяется следующее состояние;
	\item детерминированное начальное состояние - т.е начальное состояние для задачи \textbf{определено однозначным образом}; 
\end{itemize}

В задаче обучения с подкреплением мы хотим найти такую политику, которая максимизирует мат ожидание финальной награды.

Тогда политику можно представить в виде матрицы. И задача обучения с подкреплением является задачей конечномерной оптимизации.

\subsection{Алгоритм кросс энтропии}

\begin{itemize}
	\item Устанавливаем начальную политику $\pi_0$;
	\item Устанавливаем количество шагов оптимизации $N$;
	\item Устанавливаем параметр $q \in (0, 1)$ - по смыслу это параметр (\%) для отбора <<элитных>> траекторий; 
	\item Оцениваем политику (итерация по $n \in [0, N]$) (Policy evaluation):
	\begin{itemize}
		\item Устанавливаем $K$ - количество траекторий для оценки политики;
		\item Проходим по системе в соответствии с текущей политикой $K$ раз и получаем оценку мат ожидания суммарной награды: $$\mathbb{E}_{\pi_n} = \frac{1}{K}\sum_{k=1}^{K}G(\tau_k)$$
	\end{itemize}
	\item Улучшение политики (Policy improvement):
	\begin{itemize}
		\item Выбираем $q$\% <<элитных>> траекторий, т.е такие траектории награда в который больше $q$-квантиля;
		\item обновляем политику по следующему правилу: для каждого состояния и действия считаем сколько пар (действие|состояние) оказалось в <<элитных>> траекториях и делим на количество раз состояния в <<элитных>> траекториях: 
		$$\pi_{n+1}(a|s) = \frac{ \textrm{ number of pair } (a|s) \textrm{ in }\tau_n}{ \textrm{ number of } s \textrm{ in } \tau_n}$$ 
	\end{itemize}
\end{itemize}

По смыслу: у нас реализовалось несколько <<элитных>> траекторий с хорошей наградой и мы хотим обновить нашу политику таким образом, что бы она больше соответствовала тем действиям, которые реализовались в этих <<элитных>> траекториях.

Предположим, что при формировании <<элитных>> траекторий у нас в состоянии $s$ оказалось именно действие $a$, значит при обновлении политики эта дробь будет равна 1 и в следующей итерации политики вероятность из состояния $s$ совершить действие $a$ будет равно 1, т.е новая политика будет брать то действие в каждом состоянии, которое приносит максимальную награду.

\subsection{Реализация в коде}

Ниже представлена реализация алгоритма на Python.

\lstset{frame=tb,
	language=python,
	aboveskip=5mm,
	belowskip=5mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{blue},
	keywordstyle=\color{teal},
	commentstyle=\color{gray},
	stringstyle=\color{olive},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3}

\begin{lstlisting}
	state_n = 25
	action_n = 4
	
	class RandomAgent():
		def __init__(self, action_n):
			self.action_n = action_n
			return None
	
		def get_action(self, state):
			return random.randint(0, self.action_n - 1)
	
	class CEM():
		def __init__(self, state_n, action_n):
			self.state_n = state_n
			self.action_n = action_n
			self.policy = np.ones((self.state_n, self.action_n)) / self.action_n
	
		def get_action(self, state):
			return int(np.random.choice(np.arange(self.action_n), p=self.policy[state]))
	
		def update_policy(self, elite_trajectories):
			pre_policy = np.zeros((self.state_n, self.action_n))
			
			for trajectory in elite_trajectories:
				for state, action in zip(trajectory['states'], trajectory['actions']):
					pre_policy[state][action] += 1
			
			for state in range(self.state_n):
				if sum(pre_policy[state]) == 0:
					self.policy[state] = np.ones(self.action_n) / self.action_n
				else:
					self.policy[state] = pre_policy[state] / sum(pre_policy[state])
			
			return None
	
	def get_state(obs):
		return int(obs[1] * np.sqrt(state_n) + obs[0])
	
	def get_trajectory(agent, trajectory_len):
		trajectory = {'states':[], 'actions': [], 'total_reward': 0}
		
		obs = env.reset()
		state = get_state(obs)
		trajectory['states'].append(state)
		
		for _ in range(trajectory_len):
		
			action = agent.get_action(state)
			trajectory['actions'].append(action)
			
			obs, reward, done, _ = env.step(action)
			state = get_state(obs)
			trajectory['total_reward'] += reward
			
			if done:
				break
		
			trajectory['states'].append(state)
		
		return trajectory
	
	def get_elite_trajectories(trajectories, q_param):
		total_rewards = [trajectory['total_reward'] for trajectory in trajectories]
		quantile = np.quantile(total_rewards, q=q_param) 
		return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]
	
	agent = CEM(state_n, action_n)
	episode_n = 50
	trajectory_n = 100
	trajectory_len = 100
	q_param = 0.9
	
	for _ in range(episode_n):
		trajectories = [get_trajectory(agent, trajectory_len) for _ in range(trajectory_n)]
		
		mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])
		print(mean_total_reward)
		
		elite_trajectories = get_elite_trajectories(trajectories, q_param)
		
		if len(elite_trajectories) > 0:
			agent.update_policy(elite_trajectories)
	
	#test
	obs = env.reset()
	state = get_state(obs)
	
	for _ in range(trajectory_len):
	
		action = agent.get_action(state)
		
		obs, reward, done, _ = env.step(action)
		state = get_state(obs)
		
		env.render()
		time.sleep(0.5)
		
		if done:
			break
\end{lstlisting}

\section{Q-learning}

\subsection{Алгоритм Q-learning}

\subsection{Реализация в коде}

\section{SARSA}

\subsection{Алгоритм SARSA}

\subsection{Реализация в коде}

\section{Метод Монте-Карло}

\subsection{Алгоритм по методу Монте-Карло}

\subsection{Реализация в коде}



