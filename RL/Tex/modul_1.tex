\chapter{Основные алгоритмы}
\section{Основные определения}

\subsection{Марковский процесс принятия решений}
Марковский процесс принятия решений (Markov Decision Process, MDP) - это математическая модель, используемая в теории управления, искусственном интеллекте и обучении с подкреплением для моделирования ситуаций, в которых агент должен принимать решения в неопределенной среде. MDP предоставляет структуру для формализации проблем принятия решений и исследования оптимальных стратегий.

Основные компоненты MDP включают:

1. Состояния (States): Состояния представляют текущее положение или конфигурацию среды, в которой находится агент. Состояния могут быть дискретными или непрерывными, в зависимости от задачи.

2. Действия (Actions): Действия - это множество возможных действий, которые агент может выполнять в каждом состоянии. Действия также могут быть дискретными или непрерывными.

3. Функция переходов (Transition Function): Эта функция описывает вероятность перехода из одного состояния в другое после выполнения определенного действия. Она может быть представлена как $P(s'|s, a)$, где $s$ - текущее состояние, $a$ - действие, $s'$ - следующее состояние.

4. Функция вознаграждения (Reward Function): Функция вознаграждения определяет мгновенное вознаграждение, получаемое агентом при выполнении определенного действия в определенном состоянии. Обычно обозначается как $R(s, a, s')$.

5. Функция ценности (Value Function): Функция ценности определяет ожидаемую награду, которую агент может ожидать получить, начиная с определенного состояния и следуя определенной стратегии. Это позволяет агенту оценивать, насколько хорошо он справляется с задачей в различных состояниях.

6. Политика (Policy): Политика - это правило, которое определяет, какие действия агент должен предпринимать в каждом состоянии. Она может быть стохастической (вероятностной) или детерминированной.

Цель в MDP состоит в том, чтобы найти оптимальную стратегию, которая максимизирует ожидаемую сумму вознаграждений, получаемых агентом в процессе взаимодействия со средой. Это может быть достигнуто с использованием различных алгоритмов обучения с подкреплением, таких как алгоритмы Q-обучения или методы глубокого обучения (DRL).


\subsection{Политика}
В контексте обучения с подкреплением (Reinforcement Learning, RL), <<политика>> (Policy)
представляет собой стратегию или правила, которые определяют, какой выбор действий агент должен сделать
в каждом возможном состоянии среды.
Политика является центральной концепцией в RL и определяет стратегию агента для достижения его целей в задаче.

Политика может быть стохастической (вероятностной) или детерминированной:

1. Стохастическая политика: В этом случае политика определяет вероятностное распределение над действиями в каждом состоянии. Стохастическая политика полезна, когда агенту необходимо учитывать случайность в среде или в самом агенте.

2. Детерминированная политика: Детерминированная политика просто указывает одно конкретное действие в каждом состоянии. Это означает, что для данного состояния агент всегда выбирает одно и то же действие.

Цель обучения с подкреплением заключается в нахождении оптимальной политики, которая максимизирует сумму наград, получаемых агентом в процессе взаимодействия со средой. Стратегии агента могут изменяться в зависимости от различных факторов, таких как состояние среды, его опыт и обновленные оценки ценности.

Существует несколько методов для нахождения оптимальной политики, включая методы, основанные на оценке функций ценности (например, методы Q-обучения) и методы, использующие прямую оптимизацию политики (например, метод глубокого обучения с подкреплением, такие как DDPG или PPO).

Политика - это ключевой элемент в RL, поскольку она определяет, как агент взаимодействует со средой, и каким образом он выбирает действия для достижения своих целей.

В уравнениях, политика, как правило, обозначается как $\pi(a|s)$, т.е. показывает какое действие $a$ будет выполнено из состояния $s$ согласно текущей политики $\pi$.

\subsection{$v$-функция и $q$-функция}
Функция ценности состояния (State Value Function, $v$-функция, $V(s)$): Эта функция оценивает ожидаемую награду, которую агент может ожидать получить, начиная из конкретного состояния и следуя определенной стратегии. Функция $V(s)$ показывает, насколько ценно находиться в состоянии s, если агент действует согласно текущей стратегии. В математической форме, $V(s)$ представляет собой среднюю сумму будущих вознаграждений, взятых с учетом вероятностей переходов и вознаграждений.

Так же оценку $v$-функции при текущей политике $\pi$ будем обозначать $v_{\pi}(s)$.


Функция ценности действия (Action Value Function, $q$-функция, $Q(s, a)$): Эта функция оценивает ожидаемую награду, которую агент может ожидать получить, начиная из состояния s, выполняя действие a, а затем следуя определенной стратегии. Она измеряет ценность выполнения конкретного действия в конкретном состоянии при следовании текущей стратегии.

\subsubsection{Связь между $v$-функцией и $q$-функцией}
По определению мы можем увидеть связь между $v$-функцией и $q$-функцией:
$$ V(s) = \sum_{a}\pi(a|s)Q(s,a) $$

\subsection{Уравнение Беллмана}

Уравнение Беллмана для $v$-функции описывает связь между оценкой значения состояния ($v$-функции) и ожидаемыми наградами в текущем состоянии и всех возможных следующих состояниях.

Это уравнение формализует принцип оптимальности и позволяет находить оптимальные оценки значений состояний.

$$v_{\pi}(s)=\sum_{a}\pi(a|s)\Big( R(s, a) + \gamma \sum_{a'}P(s'|s, a)v_{\pi}(s') \Big)$$

Здесь:

$v(s)$ - значение $v$-функции в состоянии $s$ , т.е. ожидаемая сумма вознаграждений,
которую агент может получить, находясь в состоянии $s$.

$\pi(a|s)$ - вероятность выбора действия $a$ в состоянии $s$ при использовании стратегии $\pi$.

$P(s'|s, a)$ - вероятность перехода в состояние $s'$ и получения вознаграждения $R(s,a)$ при выполнении действия $a$ в состоянии $s$.

$\gamma$ - коэффициент дисконтирования, который определяет важность будущих вознаграждений по сравнению с немедленными вознаграждениями.


Уравнение Беллмана для $q$-функции описывает связь между оценкой значения действия из текущего состояния ($q$-функции) и ожидаемыми наградами в текущем состоянии и всех возможных следующих состояниях при всех возможных действиях.

$$q_{\pi}(s, a) = R(s, a) + \gamma \sum_{s'}P(s'|s, a)\sum_{a'}\pi(a'|s')q_{\pi}(s', a')$$


\subsection{Эпсилон-жадная политика}

Эпсилон-жадная политика (Epsilon-Greedy Policy) - это стратегия принятия решений , которая объединяет жадность (greedy) и исследование (exploration) с целью обеспечения баланса между выбором наилучших известных действий и исследованием других альтернатив.

Суть эпсилон-жадной политики заключается в следующем:

1. Агент выбирает наилучшее действие с вероятностью 1 - $\varepsilon$, где $\varepsilon$ (эпсилон) - это небольшое положительное число, обычно в интервале [0, 1], которое определяет уровень исследования агента.

2. Агент выбирает случайное действие с вероятностью $\varepsilon$. Это действие выбирается равномерно случайным образом из всех доступных действий. Это обеспечивает исследование новых стратегий и состояний среды.

По сути, эпсилон-жадная политика позволяет агенту эксплуатировать известные хорошие действия (жадность), чтобы максимизировать награду, и в то же время исследовать неизведанные действия с некоторой вероятностью (исследование), чтобы обновить свои оценки и учиться лучшим стратегиям.

Параметр $\varepsilon$ является ключевым фактором в эпсилон-жадной политике. Выбор оптимального значения $\varepsilon$ зависит от конкретной задачи и среды. Если $\varepsilon$ очень маленькое (близкое к нулю), то агент будет больше склоняться к жадной стратегии и меньше исследовать. Если $\varepsilon$ близко к единице, агент будет более склонен к случайному исследованию, нежели к эксплуатации известных действий. Выбор оптимального значения $\varepsilon$ зависит от баланса между необходимостью исследования и максимизации награды в задаче обучения с подкреплением.

Так же часто параметр $\varepsilon$ уменьшают по мере обучения агента.

Формально запишем $\varepsilon$-жадную политику следующим образом:
$$
\pi(a|s) = 
 \begin{cases}
	1 - \varepsilon + \frac{\varepsilon}{m},& \underset{a'}{\mathrm{arg max}}\, Q(s,a')\\
	\frac{\varepsilon}{m}
\end{cases}
$$

\input{cross-entropy.tex}

\input{q-learning.tex}

\section{SARSA}

\subsection{Алгоритм SARSA}

\subsection{Реализация в коде}

\section{Метод Монте-Карло}

\subsection{Алгоритм по методу Монте-Карло}

\subsection{Реализация в коде}



