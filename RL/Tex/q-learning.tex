\section{Q-learning}
Алгоритм Q-обучения (Q-Learning) - это один из популярных методов обучения с подкреплением для обучения агента принимать оптимальные действия в задаче управления. Алгоритм Q-обучения основан на идее создания и обновления таблицы значений Q, которая оценивает, насколько ценно для агента выполнение определенного действия в конкретном состоянии среды.

Вот основные шаги алгоритма Q-обучения:

1. \textbf{Инициализация Q-таблицы}: Создается таблица Q, которая представляет все возможные состояния среды и все возможные действия. Начальные значения Q устанавливаются в ноль или другие начальные значения.

2. \textbf{Выбор действия}: Агент выбирает действие на основе стратегии. На начальном этапе это может быть случайный выбор действия или другая стратегия, которая позволяет агенту исследовать среду.

3. \textbf{Взаимодействие с средой}: Агент выполняет выбранное действие и взаимодействует со средой. В результате взаимодействия агент получает награду и переходит в новое состояние.

4. \textbf{Обновление Q-значения}: Агент обновляет Q-значение для текущей пары (состояние, действие) на основе полученной награды и предыдущих оценок Q. Обновление выполняется с использованием уравнения Беллмана для Q-функции.

5. \textbf{Повторение шагов 2-4}: Агент продолжает взаимодействие со средой, выбирая действия и обновляя Q-значения до тех пор, пока не достигнет условия завершения, например, определенного количества эпох или достижения заданной производительности.

6. \textbf{Исследование и эксплуатация}: Важным аспектом Q-обучения является баланс между исследованием (выбором случайных действий для изучения среды) и эксплуатацией (выбором действий на основе текущих оценок Q для максимизации награды).

7. \textbf{Определение стратегии}: После завершения обучения агент может определить оптимальную стратегию, выбирая действия с максимальными Q-значениями для каждого состояния.

Алгоритм Q-обучения является одним из фундаментальных методов обучения с подкреплением и находит применение в различных задачах управления и обучения агентов в среде.
\subsection{Алгоритм Q-learning}
\begin{itemize}
	\item Устанавливаем начальную политику $\pi_0$;
	\item Устанавливаем количество шагов обучения $K$;
	\item Для каждого шага $t$ внутри траектории (пока не достигли конечного состояния):
	\begin{itemize}
		\item Из состояния $S_t$ действуем $A_t$, получаем награду $R_t$ и переходим в состояние $S_{t+1}$
		\item Обновляем $\varepsilon$-жадным образом политику $\pi$ и получаем следующее действие $\pi(\cdot|S_{t+1}) \to A_{t+1}$
		\item Обновляем Q-функцию:
		$$Q(S_t, A_t) \gets Q(S_t, A_t) + \alpha\big( R_t + \gamma \max_{a'}Q(S_{t+1}, a') - Q(S_t, A_t) \big)$$
	\end{itemize}
	\item Завершив шаг обучения обновляем $\varepsilon$, например $\varepsilon = \frac{1}{k}$. Также неплохо себя показала практика экспоненциального уменьшения, например $\varepsilon = e^{-\beta k}$, где $\beta$ введена для корректировки скорости убывания $\varepsilon$.
	
\end{itemize}

\subsection{Реализация в коде}

Ниже представлена реализация алгоритма на Python.

\lstset{frame=tb,
	language=python,
	aboveskip=5mm,
	belowskip=5mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=none,
	numberstyle=\tiny\color{blue},
	keywordstyle=\color{teal},
	commentstyle=\color{gray},
	stringstyle=\color{olive},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3}

\begin{lstlisting}
	state_n = 25
	action_n = 4
\end{lstlisting}